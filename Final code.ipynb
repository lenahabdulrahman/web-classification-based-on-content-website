{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eca317f",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85877a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import *\n",
    "rs=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "835a8704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>English_only_website_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>official site good hotel accommodation big sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>hotel book like use vacation work hard year lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>hotel book like previously deal predominantly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>cheap search compare find cheap flight find co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>bot create free account create free account si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                          English_only_website_text\n",
       "0        15  official site good hotel accommodation big sav...\n",
       "1        15  hotel book like use vacation work hard year lo...\n",
       "2        15  hotel book like previously deal predominantly ...\n",
       "3        15  cheap search compare find cheap flight find co...\n",
       "4        15  bot create free account create free account si..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Preprocessed_website_class_englishOnly.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfbc23a",
   "metadata": {},
   "source": [
    "# One of the websites had only one word and it is not english so the text now is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94611f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1408 entries, 0 to 1407\n",
      "Data columns (total 2 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   Category                   1408 non-null   int64 \n",
      " 1   English_only_website_text  1407 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 22.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee496706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='any', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "670b580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['English_only_website_text']\n",
    "y=df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e9c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state = rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f8edb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_evaluating_pipeline(classifier, n_gram):\n",
    "    char_tfidf = TfidfVectorizer(analyzer='char', ngram_range=n_gram)\n",
    "    word_tfidf = TfidfVectorizer(analyzer='word', ngram_range=n_gram)\n",
    "    tfidf = FeatureUnion([('char', char_tfidf), ('word', word_tfidf)])\n",
    "    pipeline = Pipeline([('tfidf', tfidf), ('clf', classifier)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866cbb9",
   "metadata": {},
   "source": [
    "# Define the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3a6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote=VotingClassifier(estimators=[('LR', LogisticRegression(random_state=rs)),('sgd', SGDClassifier()), ('LinearSVC', LinearSVC(random_state=rs))], voting='hard')\n",
    "models=[LinearSVC(random_state=rs),SVC(random_state=rs), MultinomialNB(),LogisticRegression(random_state=rs),\n",
    "        RandomForestClassifier(random_state=rs), XGBClassifier(random_state=rs), LGBMClassifier(random_state=rs), \n",
    "        DecisionTreeClassifier(random_state=rs), KNeighborsClassifier(),SGDClassifier(random_state=rs),vote]\n",
    "names=['Linear SVM', 'SVM', 'Multi-NB', 'LR', 'RF', 'XGB', 'LGBM', 'DT', 'k-NN','sgd','Voting']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ba93b",
   "metadata": {},
   "source": [
    "## Unigram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5793af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM:0.9403409090909091\n",
      "SVM:0.8693181818181818\n",
      "Multi-NB:0.6818181818181818\n",
      "LR:0.8892045454545454\n",
      "RF:0.8494318181818182\n",
      "XGB:0.8267045454545454\n",
      "LGBM:0.8380681818181818\n",
      "DT:0.6676136363636364\n",
      "k-NN:0.8664772727272727\n",
      "sgd:0.8693181818181818\n",
      "Voting:0.9176136363636364\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    print(names[i], end=':')\n",
    "    training_evaluating_pipeline(models[i],(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b2cc3",
   "metadata": {},
   "source": [
    "## Uni and bigram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e9dcb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM:0.9346590909090909\n",
      "SVM:0.8494318181818182\n",
      "Multi-NB:0.40625\n",
      "LR:0.8863636363636364\n",
      "RF:0.7897727272727273\n",
      "XGB:0.8579545454545454\n",
      "LGBM:0.8494318181818182\n",
      "DT:0.5965909090909091\n",
      "k-NN:0.8721590909090909\n",
      "sgd:0.9204545454545454\n",
      "Voting:0.90625\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    print(names[i], end=':')\n",
    "    training_evaluating_pipeline(models[i], n_gram=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cafb82b",
   "metadata": {},
   "source": [
    "## Bigram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d894845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM:0.8636363636363636\n",
      "SVM:0.7556818181818182\n",
      "Multi-NB:0.15625\n",
      "LR:0.8039772727272727\n",
      "RF:0.6988636363636364\n",
      "XGB:0.75\n",
      "LGBM:0.7869318181818182\n",
      "DT:0.45454545454545453\n",
      "k-NN:0.7045454545454546\n",
      "sgd:0.8380681818181818\n",
      "Voting:0.8295454545454546\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    print(names[i], end=':')\n",
    "    training_evaluating_pipeline(models[i], n_gram=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc2a4cc",
   "metadata": {},
   "source": [
    "## Uni, bi, and trigram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e255f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM:0.9346590909090909\n",
      "SVM:0.8494318181818182\n",
      "Multi-NB:0.4090909090909091\n",
      "LR:0.875\n",
      "RF:0.7613636363636364\n",
      "XGB:0.8267045454545454\n",
      "LGBM:0.8522727272727273\n",
      "DT:0.5397727272727273\n",
      "k-NN:0.8607954545454546\n",
      "sgd:0.9005681818181818\n",
      "Voting:0.9147727272727273\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    print(names[i], end=':')\n",
    "    training_evaluating_pipeline(models[i], n_gram=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe1c9a",
   "metadata": {},
   "source": [
    "## Bi and trigram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "468489dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM:0.9090909090909091\n",
      "SVM:0.8068181818181818\n",
      "Multi-NB:0.2755681818181818\n",
      "LR:0.8494318181818182\n",
      "RF:0.7386363636363636\n",
      "XGB:0.8153409090909091\n",
      "LGBM:0.8323863636363636\n",
      "DT:0.5454545454545454\n",
      "k-NN:0.6789772727272727\n",
      "sgd:0.9176136363636364\n",
      "Voting:0.8892045454545454\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    print(names[i], end=':')\n",
    "    training_evaluating_pipeline(models[i], n_gram=(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca344714",
   "metadata": {},
   "source": [
    "## Trigram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc95bf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM:0.9090909090909091\n",
      "SVM:0.84375\n",
      "Multi-NB:0.3096590909090909\n",
      "LR:0.8721590909090909\n",
      "RF:0.6818181818181818\n",
      "XGB:0.8068181818181818\n",
      "LGBM:0.8409090909090909\n",
      "DT:0.5454545454545454\n",
      "k-NN:0.5909090909090909\n",
      "sgd:0.9005681818181818\n",
      "Voting:0.90625\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    print(names[i], end=':')\n",
    "    training_evaluating_pipeline(models[i], n_gram=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5cf572",
   "metadata": {},
   "source": [
    "#### Based on these experiments I will optimize SVM, LR, Multi-NB, and k-NN with Unigram and Uni-Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59989dd",
   "metadata": {},
   "source": [
    "# Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683dcf2",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7fffbcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "140 fits failed out of a total of 280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "140 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.91472597        nan 0.91189578        nan 0.91095238\n",
      "        nan 0.91095238        nan 0.91              nan 0.9090566\n",
      "        nan 0.9090566         nan 0.9090566         nan 0.90811321\n",
      "        nan 0.90811321        nan 0.90716083        nan 0.90716083\n",
      "        nan 0.90716083        nan 0.90716083]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9147259658580413\n",
      "Pipeline(steps=[('tfidf',\n",
      "                 FeatureUnion(transformer_list=[('char',\n",
      "                                                 TfidfVectorizer(analyzer='char')),\n",
      "                                                ('word', TfidfVectorizer())])),\n",
      "                ('clf', LinearSVC(random_state=123))])\n"
     ]
    }
   ],
   "source": [
    "parameters = {'clf__C': [1.0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30], 'clf__penalty': ['l1', 'l2'] }\n",
    "\n",
    "char_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "word_tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1))\n",
    "tfidf = FeatureUnion([('char', char_tfidf), ('word', word_tfidf)])\n",
    "pipeline = Pipeline([('tfidf', tfidf), ('clf', LinearSVC(random_state=rs))])\n",
    "grid = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-1).fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71bfe9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM optimal score:\n",
      "* Confusion matrix: \n",
      " [[ 4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 25  0  2  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  3 23  0  1  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 29  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 21  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  3  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  0  0 29  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 21  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0 24  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0 16  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 15  1  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  1  3 15  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0 27  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 33  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26]]\n",
      "* Accuracy: 94.03\n",
      "* F1: 93.56\n",
      "* Precision: 94.49\n",
      "* Recall: 93.13\n"
     ]
    }
   ],
   "source": [
    "print('Linear SVM optimal score:')\n",
    "char_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "word_tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1))\n",
    "tfidf = FeatureUnion([('char', char_tfidf), ('word', word_tfidf)])\n",
    "pipeline = Pipeline([('tfidf', tfidf), ('clf', LinearSVC(random_state=123))])\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n",
    "print('* Confusion matrix: \\n', confusion_matrix(y_test, predictions))\n",
    "print('* Accuracy: %0.2f' %(accuracy_score(y_test, predictions)*100))\n",
    "print('* F1: %0.2f' %(f1_score(y_test, predictions, average='macro')*100))\n",
    "print('* Precision: %0.2f' %(precision_score(y_test, predictions, average='macro')*100))\n",
    "print('* Recall: %0.2f' %(recall_score(y_test, predictions, average='macro')*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7d5c0",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f7f2e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8891015274034142\n",
      "Pipeline(steps=[('tfidf',\n",
      "                 FeatureUnion(transformer_list=[('char',\n",
      "                                                 TfidfVectorizer(analyzer='char')),\n",
      "                                                ('word', TfidfVectorizer())])),\n",
      "                ('clf',\n",
      "                 SGDClassifier(max_iter=50, penalty=None, random_state=123))])\n"
     ]
    }
   ],
   "source": [
    "parameters = {\"clf__penalty\": [\"l2\", \"elasticnet\",\"l1\", None ], 'clf__max_iter': [50, 80, 100, 500, 1000]}\n",
    "\n",
    "char_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "word_tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1))\n",
    "tfidf = FeatureUnion([('char', char_tfidf), ('word', word_tfidf)])\n",
    "pipeline = Pipeline([('tfidf', tfidf), ('clf', SGDClassifier(random_state=rs))])\n",
    "grid = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-1).fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9931020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD optimal score:\n",
      "* Confusion matrix: \n",
      " [[ 4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 24  1  2  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  1 24  0  1  0  0  0  0  0  0  0  1  1  0  0]\n",
      " [ 0  0  0 28  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  1  0  0  0  0  3  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  1  0 29  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 21  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0 23  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0 16  0  1  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0 15  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  1  3 15  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0 27  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0 32  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0 25]]\n",
      "* Accuracy: 92.61\n",
      "* F1: 92.38\n",
      "* Precision: 93.25\n",
      "* Recall: 91.93\n"
     ]
    }
   ],
   "source": [
    "print('SGD optimal score:')\n",
    "char_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "word_tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1))\n",
    "tfidf = FeatureUnion([('char', char_tfidf), ('word', word_tfidf)])\n",
    "pipeline = Pipeline([('tfidf', tfidf), ('clf', SGDClassifier(max_iter=50, penalty=None, random_state=123))])\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n",
    "print('* Confusion matrix: \\n', confusion_matrix(y_test, predictions))\n",
    "print('* Accuracy: %0.2f' %(accuracy_score(y_test, predictions)*100))\n",
    "print('* F1: %0.2f' %(f1_score(y_test, predictions, average='macro')*100))\n",
    "print('* Precision: %0.2f' %(precision_score(y_test, predictions, average='macro')*100))\n",
    "print('* Recall: %0.2f' %(recall_score(y_test, predictions, average='macro')*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10a46e",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52cfb5a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "300 fits failed out of a total of 1200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.9090566  0.90330638 0.90526505 0.90999102        nan        nan\n",
      "        nan 0.87769991 0.91095238 0.91095238 0.91189578 0.91\n",
      " 0.9090566  0.90330638 0.90526505 0.90999102        nan        nan\n",
      "        nan 0.7734681  0.89291105 0.89291105 0.89291105 0.89291105\n",
      " 0.9090566  0.90330638 0.90526505 0.90999102        nan        nan\n",
      "        nan 0.83599281 0.9033513  0.9033513  0.9033513  0.9033513\n",
      " 0.9090566  0.90330638 0.90526505 0.90999102        nan        nan\n",
      "        nan 0.85491465 0.90619048 0.90619048 0.90619048 0.90619048\n",
      " 0.9090566  0.90330638 0.90526505 0.90999102        nan        nan\n",
      "        nan 0.85777179 0.90809524 0.90809524 0.90809524 0.90809524\n",
      " 0.9090566  0.90330638 0.90526505 0.90999102        nan        nan\n",
      "        nan 0.86250674 0.90904762 0.90904762 0.90904762 0.90904762\n",
      " 0.9090566  0.90330638 0.90526505 0.90999102        nan        nan\n",
      "        nan 0.86916442 0.9109434  0.9109434  0.90904762 0.90904762\n",
      " 0.9090566  0.90330638 0.90526505 0.90999102        nan        nan\n",
      "        nan 0.87106918 0.91189578 0.91189578 0.91       0.91\n",
      " 0.9090566  0.90330638 0.90526505 0.90999102        nan        nan\n",
      "        nan 0.87580413 0.91189578 0.91189578 0.9090566  0.9090566\n",
      " 0.9090566  0.90330638 0.90526505 0.90999102        nan        nan\n",
      "        nan 0.87486074 0.91096137 0.91096137 0.91000898 0.91000898]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9118957771787961\n",
      "Pipeline(steps=[('tfidf',\n",
      "                 FeatureUnion(transformer_list=[('char',\n",
      "                                                 TfidfVectorizer(analyzer='char')),\n",
      "                                                ('word', TfidfVectorizer())])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=10, random_state=123, solver='sag'))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "parameters = {'clf__penalty':[None, 'l1', 'l2'], 'clf__solver' :['newton-cg', 'lbfgs', 'sag', 'saga'], 'clf__C':[ 10, 1.0, 2, 3, 4, 5, 6, 7, 8, 9]}\n",
    "\n",
    "char_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "word_tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1))\n",
    "tfidf = FeatureUnion([('char', char_tfidf), ('word', word_tfidf)])\n",
    "pipeline = Pipeline([('tfidf', tfidf), ('clf', LogisticRegression(random_state=rs))])\n",
    "grid = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-1).fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f5f6e71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR optimal score:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muner\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Confusion matrix: \n",
      " [[ 4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 27  0  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  3 23  0  1  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 29  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0 20  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  3  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  0  0 29  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 21  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0 23  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0 16  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 16  0  0  0  0]\n",
      " [ 0  1  1  0  0  0  0  0  0  0  1  3 14  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0 27  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 33  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0 25]]\n",
      "* Accuracy: 93.75\n",
      "* F1: 93.38\n",
      "* Precision: 94.76\n",
      "* Recall: 92.86\n"
     ]
    }
   ],
   "source": [
    "print('LR optimal score:')\n",
    "char_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "word_tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1))\n",
    "tfidf = FeatureUnion([('char', char_tfidf), ('word', word_tfidf)])\n",
    "pipeline = Pipeline([('tfidf', tfidf), ('clf', LogisticRegression(C=10, random_state=123, solver='sag'))])\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n",
    "print('* Confusion matrix: \\n', confusion_matrix(y_test, predictions))\n",
    "print('* Accuracy: %0.2f' %(accuracy_score(y_test, predictions)*100))\n",
    "print('* F1: %0.2f' %(f1_score(y_test, predictions, average='macro')*100))\n",
    "print('* Precision: %0.2f' %(precision_score(y_test, predictions, average='macro')*100))\n",
    "print('* Recall: %0.2f' %(recall_score(y_test, predictions, average='macro')*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
